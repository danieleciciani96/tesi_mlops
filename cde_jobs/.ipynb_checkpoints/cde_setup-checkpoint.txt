### Setuo cde
mkdir .cde

nano .cde/config.yaml
user: dciciani
vcluster-endpoint: https://glhpck7s.cde-8fhkqc8g.ps-sandb.a465-9q4k.cloudera.site/dex/api/v1

# Manually upload the CDE CLI before running the below commands:
cp lib/cde /home/cdsw/.local/bin
chmod 777 /home/cdsw/.local/bin/cde

### create Python CDE Resources
cde resource create --name python-res-mlops --type python-env --python-version python3
cde resource list --filter name[rlike]python-res-mlops


# Upload the requirements file for the cde python env
cde resource upload --name python-res-mlops --local-path ${HOME}/cde_jobs/requirements.txt --resource-path requirements.txt

# Check the status of the environment
cde resource list-events --name python-res-mlops

### Create a File Resource
cde resource create --name file-res-mlops --type files

# upload files into Resource
cde resource upload --name file-res-mlops --local-path cde_jobs/0_refresh_raw.py
cde resource upload --name file-res-mlops --local-path cde_jobs/mlops_airflow_dag.py


# Create the SPARK job refresh_raw
cde job create --type spark \
               --python-env-resource-name python-res-mlops \
               --mount-1-resource file-res-mlops \
               --application-file 0_refresh_raw.py \
               --name 0_refresh_raw

# Update the SPARK job refresh_raw
cde job update --mount-1-resource file-res-mlops \
               --application-file 0_refresh_raw.py \
               --name 0_refresh_raw

# Create the AIRFLOW Job CICD_Pipeline
cde job create --type airflow \
               --dag-file mlops_airflow_dag.py \
               --mount-1-resource file-res-mlops \
               --name cicd_pipeline_mlops
               
               
### Recover a previous version
cde resource upload --name file-res-mlops --local-path cde_jobs/mlops_airflow_dag_recover.py

cde job create --type airflow \
               --dag-file mlops_airflow_dag_recover.py \
               --mount-1-resource file-res-mlops \
               --name cicd_pipeline_mlops_recover
